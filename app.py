import streamlit as st
import pandas as pd

df_login_agg_date = pd.read_csv(r"tables\login_agg_date.csv", sep=",", index_col=0)
df_first_time_login = pd.read_csv(
    r"tables/first_time_login.csv",
    sep=",",
    index_col=0,
    keep_default_na=False,
    na_values=[],  # evita interpretaÃ§Ã£o automÃ¡tica de valores vazios
    dtype=str,  # forÃ§a tudo como string para nÃ£o perder nada
    encoding="utf-8"
)


# Controle de navegaÃ§Ã£o entre as seÃ§Ãµes interativas
if "page" not in st.session_state:
    st.session_state.page = "ğŸ“Œ Overview"

# FunÃ§Ã£o para mudar de pÃ¡gina
def change_page(page_name):
    st.session_state.page = page_name
    st.rerun()


# Sidebar
st.sidebar.title("NavegaÃ§Ã£o")
nav = st.sidebar.radio("Ir para:", ["ğŸ“Œ Overview", "â˜ï¸ Ambiente na Nuvem", "ğŸ–¥ï¸ Ambiente Local", 
                                    "ğŸ—ï¸ Arquitetura de Dados"],
                                    index=["ğŸ“Œ Overview", "â˜ï¸ Ambiente na Nuvem", "ğŸ–¥ï¸ Ambiente Local", 
                                    "ğŸ—ï¸ Arquitetura de Dados"].index(st.session_state.page))

# Atualiza a pÃ¡gina ativa com base no radio button
if nav != st.session_state.page:
    st.session_state.page = nav
    st.rerun()

# PÃ¡gina: Overview
if st.session_state.page == "ğŸ“Œ Overview":


    st.title("ğŸ“Œ VisÃ£o Geral do Projeto de Pipeline de Dados")

    st.markdown("""
    Este projeto implementa um pipeline completo para a simulaÃ§Ã£o e anÃ¡lise de logins de usuÃ¡rios em um jogo. Para testar sua execuÃ§Ã£o, siga os passos de configuraÃ§Ã£o do ambiente local e cloud, descritos nas prÃ³ximas seÃ§Ãµes.

    ApÃ³s a conclusÃ£o do setup, a DAG estarÃ¡ pronta para ser executada. Em um cenÃ¡rio de produÃ§Ã£o, as DAGs devem ser agendadas para execuÃ§Ã£o diÃ¡ria, garantindo que os dados estejam sempre atualizados.

    A execuÃ§Ã£o do pipeline se inicia manualmente atravÃ©s da DAG `dag_data_creation`. A DAG `dag_data_transformation` foi desenvolvida para iniciar automaticamente ao tÃ©rmino da primeira.

    O projeto foi estruturado visando mÃ¡xima automaÃ§Ã£o e tolerÃ¢ncia a falhas. Para isso, foram implementadas notificaÃ§Ãµes no Slack em casos de sucesso ou falha, enviadas para um canal especÃ­fico de logs. Essa abordagem proporciona visibilidade operacional em ambientes com mÃºltiplas DAGs, permitindo respostas rÃ¡pidas e eficazes diante de falhas.

    VisualizaÃ§Ã£o das notificaÃ§Ãµes:
    """)

    st.image("images/logs_slack.png", caption="NotificaÃ§Ãµes via Slack - Airflow", use_container_width=True)

    st.markdown("---")


    st.markdown("""
    ### ğŸ¯ Objetivo
    O principal objetivo do projeto foi disponibilizar tabelas analÃ­ticas que possam ser consumidas por analistas para gerar insights relevantes que apoiem a tomada de decisÃµes.

    A arquitetura foi projetada para ser escalÃ¡vel, considerando o crescimento contÃ­nuo da base de usuÃ¡rios e volume de dados. Por isso, optou-se pelo uso de tecnologias como PySpark e SparkSQL, que permitem processar grandes volumes de dados de forma eficiente.
    """)

    st.markdown("---")

    st.markdown("""
    ### ğŸ§¾ Tabelas Finais Geradas

    - **`login_agg_date`**  
      Tabela agregada por data. Como os registros originais de login sÃ£o baseados em timestamp, foi realizada uma agregaÃ§Ã£o por data, contabilizando o nÃºmero de logins por usuÃ¡rio a cada dia. TambÃ©m foram incluÃ­das as informaÃ§Ãµes de continente, paÃ­s e sistema operacional utilizados.

    """)
    st.dataframe(df_login_agg_date)

    st.markdown("""
    - **`first_time_login`**  
      Esta tabela apresenta uma agregaÃ§Ã£o por `user_id`, contendo a quantidade total de logins realizados por cada usuÃ¡rio, bem como a data e o horÃ¡rio do primeiro login registrado. AlÃ©m disso, inclui as informaÃ§Ãµes de continente, paÃ­s e sistema operacional.

    """)
    st.dataframe(df_first_time_login)

    st.markdown("""
    ReforÃ§ando: em um ambiente de produÃ§Ã£o, Ã© essencial que as DAGs sejam executadas diariamente, garantindo a atualizaÃ§Ã£o contÃ­nua das tabelas analÃ­ticas.
    """)

    st.markdown("---")

    st.subheader("ğŸ“ Estrutura de Pastas do Projeto")

    st.code('''
    â”œâ”€â”€ data_pipeline_project/
    â”‚   â”œâ”€â”€ dags/
    â”‚   â”‚   â”œâ”€â”€ dag_data_creation.py
    â”‚   â”‚   â””â”€â”€ dag_data_transformation.py
    â”‚   â”œâ”€â”€ spark-scripts/
    â”‚   â”‚   â”œâ”€â”€ create/
    â”‚   â”‚   â”‚   â”œâ”€â”€ logins.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ user_country.py
    â”‚   â”‚   â”‚   â””â”€â”€ user_info.py
    â”‚   â”‚   â””â”€â”€ transform-load/
    â”‚   â”‚       â”œâ”€â”€ first_time_login.py
    â”‚   â”‚       â””â”€â”€ login_agg_date.py
    â”‚   â”œâ”€â”€ sql-scripts/
    â”‚   â”‚   â”œâ”€â”€ copy_table/
    â”‚   â”‚   â”‚   â”œâ”€â”€ copy_first_time_login.sql
    â”‚   â”‚   â”‚   â””â”€â”€ copy_login_agg_date.sql
    â”‚   â”‚   â””â”€â”€ create_table/
    â”‚   â”‚       â”œâ”€â”€ first_time_login.sql
    â”‚   â”‚       â”œâ”€â”€ login_agg_date.sql
    â”‚   â”‚       â”œâ”€â”€ logins.sql
    â”‚   â”‚       â”œâ”€â”€ user_country.sql
    â”‚   â”‚       â””â”€â”€ user_info.sql
    â”‚   â”œâ”€â”€ utilitys/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ connections.py
    â”‚   â”‚   â””â”€â”€ slack_notifications.py
    â”‚   â”œâ”€â”€ .env
    â”‚   â”œâ”€â”€ docker-compose.yml
    â”‚   â”œâ”€â”€ Dockerfile.airflow
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â””â”€â”€ README.md
    ''')

    st.markdown("---")
    st.subheader("âœ… Pronto para iniciar?")
    if st.button("â˜ï¸ Configurar Ambiente na Nuvem"):
        change_page("â˜ï¸ Ambiente na Nuvem")
        st.rerun()

# PÃ¡gina: Ambiente na Nuvem
elif st.session_state.page == "â˜ï¸ Ambiente na Nuvem":
    st.title("â˜ï¸ Setup: Ambiente na Nuvem (AWS)")

    st.markdown("""
## âœï¸ IntroduÃ§Ã£o

Este documento apresenta os passos necessÃ¡rios para configurar e executar o projeto na AWS. O processo envolve trÃªs aspectos principais: estabelecer a conexÃ£o local com o Redshift Serverless, criar os buckets no S3 e definir suas respectivas polÃ­ticas e configuraÃ§Ãµes de seguranÃ§a para permitir o acesso do Redshift aos buckets.

A adoÃ§Ã£o desses passos se deve Ã  escolha de desenvolver os cÃ³digos utilizando PySpark. Inicialmente, utilizei a conexÃ£o direta com o Redshift por meio do driver JDBC fornecido pela AWS. Embora essa abordagem permitisse utilizar exclusivamente o Redshift, sem a necessidade de outros serviÃ§os da AWS, ela apresenta uma limitaÃ§Ã£o importante, o Redshift Serverless Ã© tarifado por consulta e tempo de execuÃ§Ã£o, e o uso do JDBC faz com que o PySpark envie os dados linha por linha. Essa prÃ¡tica, alÃ©m de extremamente ineficiente do ponto de vista de desempenho, resulta em um custo significativamente mais elevado devido ao tempo de execuÃ§Ã£o para salvar os dados.

Diante disso, optei por uma abordagem mais eficiente: salvar os dados no S3 em formato Parquet, um formato muito leve e otimizado que o PySpark consegue utilizar seu processamento distribuÃ­do para o formato e, em seguida, utilizar o comando COPY para transferi-los para o Redshift. Essa estratÃ©gia Ã© vantajosa por dois motivos principais, o Redshift Ã© nativamente estruturado para realizar o COPY com processamento paralelo, o que garante um desempenho muito superior, e por ser mais rÃ¡pido, diminui significativamente o tempo que o RedShift fica em execuÃ§Ã£o, contribuindo diretamente para a reduÃ§Ã£o de custos.

Portanto, os passos descritos a seguir foram definidos com o objetivo de garantir maior eficiÃªncia, desempenho e economia na execuÃ§Ã£o do projeto.

---

## âœ… Passo a Passo de ConfiguraÃ§Ã£o

### 1. Criar o Workgroup e Namespace no Redshift Serverless

- Acesse o console do Amazon Redshift â†’ Redshift Serverless.
- Crie um novo **Workgroup** e um **Namespace**.
- Associe uma **funÃ§Ã£o IAM** com permissÃµes para acessar o S3 (detalhado no passo 4).
- Acesse o Redshift â†’ Workgroup â†’ Editar configuraÃ§Ãµes.
- Ative a opÃ§Ã£o Publicamente acessÃ­vel.

### 2. Criar o Bucket no Amazon S3

- Acesse o console do Amazon S3 e crie um bucket (ex: `final-data-game`).
- No bucket crie duas pastas (ex: `login-agg-date` e `first-time-login`).

### 3. Configurar o Grupo de SeguranÃ§a (Security Group)

- VÃ¡ atÃ© o **EC2 â†’ Security Groups**.
- Localize o grupo de seguranÃ§a associado ao seu Workgroup Redshift.
- Adicione uma **regra de entrada**:
    - Tipo: PostgreSQL
    - Protocolo: TCP
    - Porta: 5439
    - Origem: My IP

### 4. Criar e Configurar a FunÃ§Ã£o IAM

- Acesse o **IAM â†’ Roles â†’ Create Role**.
- Tipo de entidade confiÃ¡vel: **AWS Service**  
- ServiÃ§o: **Redshift**  
- Tipo: **Redshift - Customizable**
- Adicione a polÃ­tica `AmazonS3ReadOnlyAccess`
- Finalize a criaÃ§Ã£o da role e copie o ARN.
- Salve o ARN pois irÃ¡ ser utilizado quando for realizar o setup do ambiente local

### 5. Configurar PermissÃµes no Bucket S3
- Acesse o bucket no S3 â†’ Aba PermissÃµes â†’ PolÃ­tica do bucket.
- Adicione a polÃ­tica abaixo (substitua os valores adequadamente):
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowRedshiftServerlessReadAccess",
      "Effect": "Allow",
      "Principal": {
        "Service": "redshift.amazonaws.com"
      },
      "Action": [
        "s3:GetObject",
        "s3:ListBucket",
        "s3:PutObject",
        "s3:PutObjectAcl"
      ],
      "Resource": [
        "arn:aws:s3:::final-data-game",
        "arn:aws:s3:::final-data-game/*"
      ],
      "Condition": {
        "StringEquals": {
          "aws:SourceAccount": "YOUR_ACCOUNT_ID"
        },
        "ArnLike": {
          "aws:SourceArn": "YOUR_REDSHIFT_WORKGROUP_ARN"
        }
      }
    }
  ]
}

    """)

    st.markdown("---")
    st.subheader("ğŸ§© Pronto para configurar o ambiente local?")
    if st.button("ğŸ–¥ï¸ Ir para Ambiente Local"):
        change_page("ğŸ–¥ï¸ Ambiente Local")
        st.rerun()


# PÃ¡gina: Ambiente Local
elif st.session_state.page == "ğŸ–¥ï¸ Ambiente Local":
    st.title("ğŸ–¥ï¸ Setup: Ambiente Local")

    st.markdown("""
    ### ğŸ“„ 1. Clonar o RepositÃ³rio
    ```git
    git clone https://github.com/IgorCanelo/ETL.git
    ```      
    ### ğŸ” 2. ConfiguraÃ§Ã£o do `.env`
    Crie um arquivo chamado de .env na raiz do projeto e preencha com as credenciais necessÃ¡rias para testes locais.
                
    ```dotenv
    AWS_ACCESS_KEY_ID=your_access_key_aws
    AWS_SECRET_ACCESS_KEY=your_secret_key_aws
    SAVE_TABLE_1_S3=s3a://final-data-game/first-time-login/
    SAVE_TABLE_2_S3=s3a://final-data-game/login-agg-date/
    HOST_REDSHIFT=your_redshift_host
    SCHEMA_REDSHIFT=your_schema
    LOGIN_REDSHIFT=your_login
    PASSWORD_REDSHIFT=your_password
    SLACK_WEBHOOK_URL=your_webhook_slack
    ```
                
    ### ğŸ” 3. ConfiguraÃ§Ã£o dos arquivos localizados em `sql-scripts/copy_table`
    - Para os dois arquivos:
        - `COPY` - Se os buckets e pastas foram criados com os nomes sugeridos nÃ£o Ã© necessÃ¡rio atualizar
        - `FROM` - Cole o seu ARN obtido do RedShift
                

    ### ğŸ³ 4. Build do Docker
    ```bash
    docker build -f Dockerfile.airflow -t data_pipeline_project .
    ```

    ### ğŸ”„ 5. Subir os containers com Docker Compose
    ```bash
    docker-compose up
    ```

    ### ğŸŒ 6. Acessar o Airflow
    - Acesse em: [http://localhost:8080](http://localhost:8080)
                
    ### ğŸ§ª 7. (Opcional) Conectar via DBeaver

    Para facilitar a visualizaÃ§Ã£o e execuÃ§Ã£o de queries no banco, recomenda-se usar o **DBeaver**:

    1. FaÃ§a o download e instale o DBeaver: [https://dbeaver.io/download/](https://dbeaver.io/download/)
    2. Crie duas conexÃµes:
        - Uma para o **PostgreSQL local**:
            - Host: `localhost`
            - Porta: `5433`
            - Database: `game_data`
            - UsuÃ¡rio: `game`
            - Senha: `game123`
        - Outra para o **Redshift Serverless**:
            - Host: `HOST_REDSHIFT` (do `.env`)
            - Porta: `5439`
            - Database: conforme configurado no Redshift
            - UsuÃ¡rio/Senha: conforme definido no `.env`

    ğŸ’¡ *Essa etapa Ã© opcional, mas Ãºtil para debug e anÃ¡lise dos dados carregados.*

    """)
    
    st.markdown("---")
    if st.button("ğŸ—ï¸ Ver Arquitetura sugerida para uma empresa de jogos mobile"):
        change_page("ğŸ—ï¸ Arquitetura de Dados")
        st.rerun()

# PÃ¡gina: Detalhes do Projeto

    

# PÃ¡gina: Arquitetura de Dados
elif st.session_state.page == "ğŸ—ï¸ Arquitetura de Dados":
    st.title("ğŸ—ï¸ Arquitetura sugerida de Dados")

    st.markdown("""
    ### Arquitetura de Dados

    A arquitetura foi desenhada para ser **escalÃ¡vel**, **modular** e de **fÃ¡cil manutenÃ§Ã£o**.

    ---

    #### âœ… PrÃ³s

    - **SeparaÃ§Ã£o clara por camadas (Raw, Bronze, Silver, Gold):**  
      Facilita o tratamento e a evoluÃ§Ã£o da qualidade dos dados em cada etapa, promovendo um pipeline mais confiÃ¡vel e organizado.

    - **Uso do Airflow para orquestraÃ§Ã£o:**  
      Ferramenta robusta e amplamente adotada para agendamento e monitoramento de workflows. Permite automaÃ§Ãµes complexas com boa visibilidade. Ã‰ open-source e possui uma grande comunidade.

    - **IngestÃ£o de dados com Airbyte:**  
      Oferece conectores prontos para diversas fontes, acelerando a ingestÃ£o com baixo esforÃ§o de configuraÃ§Ã£o. TambÃ©m Ã© open-source, permitindo customizaÃ§Ãµes.

    - **Armazenamento escalÃ¡vel no Data Lake (S3):**  
      O Amazon S3 proporciona escalabilidade, durabilidade e flexibilidade para dados em diferentes formatos e volumes.

    - **Redshift como Data Warehouse:**  
      Eficiente para anÃ¡lises em larga escala, com boa integraÃ§Ã£o ao ecossistema AWS.

    - **VisualizaÃ§Ã£o com Power BI:**  
      Entrega insights de forma amigÃ¡vel ao usuÃ¡rio final, com integraÃ§Ã£o fluida com o Redshift e outras fontes.

    - **Observabilidade com CloudWatch e Slack:**  
      Permite monitoramento centralizado e alertas automÃ¡ticos, agilizando a detecÃ§Ã£o e resoluÃ§Ã£o de falhas.

    ---

    #### âš ï¸ Contras

    - **Gerenciamento de ferramentas auto-hospedadas (Airflow e Airbyte):**  
      Exigem operaÃ§Ã£o e manutenÃ§Ã£o constantes, o que pode aumentar a complexidade operacional em cenÃ¡rios mais exigentes.

    - **Custo e uso eficiente de recursos:**  
      O uso de serviÃ§os como EC2 e Redshift pode gerar custos elevados se nÃ£o houver governanÃ§a. Ã‰ necessÃ¡rio gerenciar o tempo de execuÃ§Ã£o das instÃ¢ncias para evitar gastos desnecessÃ¡rios.

    - **Curva de aprendizado:**  
      As ferramentas escolhidas sÃ£o poderosas, mas possuem curva de aprendizado significativa, podendo demandar mais tempo na implementaÃ§Ã£o e onboarding da equipe.

    - **Monitoramento e logging distribuÃ­do:**  
      Logs e mÃ©tricas espalhados entre diferentes ferramentas (Airflow, Airbyte, CloudWatch) podem dificultar a centralizaÃ§Ã£o e o troubleshooting em pipelines complexos.
    """)

    # Comentei a linha abaixo pois a imagem pode nÃ£o existir
    st.image("images/arquitetura.png", caption="Diagrama da Arquitetura", use_container_width=True)
    
    st.markdown("---")
    if st.button("ğŸ“Œ Voltar para Overview"):
        change_page("ğŸ“Œ Overview")
        st.rerun()